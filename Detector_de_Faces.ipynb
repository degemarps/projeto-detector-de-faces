{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detector de Faces.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d66FdvQ1GAdt"
      },
      "source": [
        "Para iniciar os experimentos, precisamos construir a nossa base de dados. Vamos utilizar como exemplo a base CALTECH-101 (http://www.vision.caltech.edu/Image_Datasets/Caltech101/Caltech101.html).\n",
        "Essa base de dados contém 101 objetos diferentes com um conjunto de 40 a 800 imagens por categoria. A categoria que escolhemos para trabalharmos no nosso detector foi a `Faces`, que contém imagens de rostos de pessoas.\n",
        "Sabendo que a base do detector de objetos é um classificador, precisamos de exemplos do que são esses objetos e exemplos do que não consiste no objeto de interesse. Para isso vamos usar a 13 Natural Scene Category (http://vision.stanford.edu/resources_links.html#datasets).\n",
        "\n",
        "Vamos primeiro especificar os caminhos para os nossos dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCY0bqS6ZiNl"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "conf = {\n",
        "    'faces_dataset': '/gdrive/MyDrive/Mestrado/VisaoComputacional/Faces',\n",
        "    'image_data': '/gdrive/MyDrive/Mestrado/VisaoComputacional/Faces_Annotations',\n",
        "    'image_distractions': '/gdrive/MyDrive/Mestrado/VisaoComputacional/sceneclass13'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IylOlfCVJ6-c"
      },
      "source": [
        "Vamos analisar o tamanho médio dos objetos e seu *aspect ratio* médio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j8t2-QKeYVi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3e2dd36-9a09-46bb-cb73-d4b34514a2d8"
      },
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "from scipy import io\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "widths = []\n",
        "heights = []\n",
        "\n",
        "for p in glob.glob(conf[\"image_data\"] + \"/*.mat\"):\n",
        "\t(y, h, x, w) = io.loadmat(p)[\"box_coord\"][0]\n",
        "\twidths.append(w - x)\n",
        "\theights.append(h - y)\n",
        "\n",
        "(avgWidth, avgHeight) = (np.mean(widths), np.mean(heights))\n",
        "print(\"[INFO] largura média: {:.2f}\".format(avgWidth))\n",
        "print(\"[INFO] altura média: {:.2f}\".format(avgHeight))\n",
        "print(\"[INFO] aspect ratio: {:.2f}\".format(avgWidth / avgHeight))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] largura média: 204.24\n",
            "[INFO] altura média: 263.34\n",
            "[INFO] aspect ratio: 0.78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT5ESyrCKNAl"
      },
      "source": [
        "Vamos agora construir uma função que nos ajudará a extrair a região de interesse da imagem que fica dentro do *bounding box*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nky-l_YkoBrX"
      },
      "source": [
        "def roi_extract(image, bb, padding=10, dstSize=(42, 42)):\n",
        "\t# com base no bounding box extrai o ROI da imagem\n",
        "\n",
        "\t(y, h, x, w) = bb\n",
        "\t(x, y) = (max(x - padding, 0), max(y - padding, 0))\n",
        "\troi = image[y:h + padding, x:w + padding]\n",
        "\t# redimensiona a região de interesse para um tamanho desejado, se for o caso.\n",
        "\troi = cv2.resize(roi, dstSize, interpolation=cv2.INTER_AREA)\n",
        "\t# retorna somente a região de interesse da imagem.\n",
        "\treturn roi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuR9AJRlKgFk"
      },
      "source": [
        "Uma das técnicas que será utilizada para detectar os objetos será por meio das pirâmide de imagens.\n",
        "\n",
        "Para isso vamos implementar duas funções para contruir essa pirâmide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lN1Dw0AcWNZ"
      },
      "source": [
        "def resize(image, width=None, height=None, inter=cv2.INTER_AREA):\n",
        "\t\n",
        "  # inicializa as dimensões da imagem e obtém o tamanho da imagem original.\n",
        "\tdim = None\n",
        "\t(h, w) = image.shape[:2]\n",
        "\t# se ambas a altura e largura forem None, retorna a imagem original.  \n",
        "\tif width is None and height is None:\n",
        "\t\treturn image\n",
        "\t\n",
        "    # calcula o AR com base na altura.\n",
        "\tif width is None:\t\t\n",
        "\t\tr = height / float(h)\n",
        "\t\tdim = (int(w * r), height)\n",
        "\t# caso contrário, calcula o AR com base na largura.\n",
        "\telse:\n",
        "\t\tr = width / float(w)\n",
        "\t\tdim = (width, int(h * r))\n",
        "\t\n",
        "  # redimensiona a imagem.\n",
        "\tresized = cv2.resize(image, dim, interpolation=inter)\n",
        "\t\n",
        "  # retorna a imagem redimensionada.\n",
        "\treturn resized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmRfz_pOb70R"
      },
      "source": [
        "def pyramid(image, scale=2.5, minSize=(40, 40)):\n",
        "\t# retorna a imagem original\n",
        "\tyield image\n",
        "\t\n",
        "  # continua o loop pela pirâmide\n",
        "\twhile True:\n",
        "\t\t# computa as novas dimensões da imagem e a redimensiona    \n",
        "\t\tw = int(image.shape[1] / scale)\n",
        "\t\timage = resize(image, width=w)\n",
        "\t\t\n",
        "    # verifica se o tamanho da imagem redimensinada atinge é menor que o tamanho\n",
        "    # mínimo fornecido. \n",
        "\t\tif image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
        "      # para a construção da pirâmide\n",
        "\t\t\tbreak\n",
        "\t\t\n",
        "    # retorna a próxima imagem da pirâmide\n",
        "\t\tyield image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L9MhLswYdDM"
      },
      "source": [
        "Para detectar um objeto, precisamos buscar dentro da imagem por esse objeto.\n",
        "\n",
        "Para isso vamos implementar um esquema de janelas deslizantes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_CV5WtFcH3G"
      },
      "source": [
        "def sliding_window(image, stepSize, windowSize):\n",
        "\t# move a janela através da imagem\n",
        "\tfor y in range(0, image.shape[0], stepSize):\n",
        "\t\tfor x in range(0, image.shape[1], stepSize):\n",
        "\t\t\t# retorna a porção da imagem referente a janela corrente\n",
        "\t\t\tyield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqtggxy1ZC4A"
      },
      "source": [
        "Para extrair as características de cada recorte feito pelas janelas deslizantes, precisamos, primeiro, estruturar os dados de uma maneira que possam ser facilmente acessados e armazenados.\n",
        "\n",
        "Vamos criar duas funções iniciais para ajudar nisso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1Xb9Sx9a61X"
      },
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "def dump_dataset(data, labels, path, datasetName, writeMethod=\"w\"):\n",
        "  # abre banco de dados, cria um dataset e escreve os dados e rótulos.  \n",
        "  db = h5py.File(path, writeMethod)\n",
        "  dataset = db.create_dataset(datasetName, (len(data), len(data[0]) + 1), dtype=\"float\")\n",
        "  dataset[0:len(data)] = np.c_[labels, data]\n",
        "  db.close()\n",
        " \n",
        "def load_dataset(path, datasetName):\n",
        "  # abre o banco de dados, coleta os dados e rótulos.\n",
        "  db = h5py.File(path, \"r\")\n",
        "  (labels, data) = (db[datasetName][:, 0], db[datasetName][:, 1:])\n",
        "  db.close()\n",
        "  # retorna a tupla de dados e rótulos\n",
        "  return (data, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1PKKoE1Z--O"
      },
      "source": [
        "Agora vamos juntar tudo para extrair as características das imagens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81gXRvR5DazU"
      },
      "source": [
        "from imutils import paths\n",
        "import progressbar\n",
        "import skimage\n",
        "from skimage import feature\n",
        "import random\n",
        "\n",
        "# parâmetro referente ao percentual de utilização da base para treinamento\n",
        "conf['percent_gt_images'] = 0.5\n",
        "# tamanho do padding (quantidade de pixels) a ser utilizado no corte da imagem\n",
        "# é sempre bom deixar uns pixels a mais nas bordas da imagem\n",
        "conf['offset'] = 5\n",
        "# tamanho da janela deslizante\n",
        "conf['window_dim'] = [100, 110]\n",
        "# número de orientações do descritor HOG\n",
        "conf['orientations'] = 9\n",
        "# pixels por célula do HOG\n",
        "conf['pixels_per_cell'] = [8, 8]\n",
        "# células por bloco do HOG\n",
        "conf['cells_per_block'] = [4, 4]\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# obtém um conjunto de referência para o treinamento (ground-truth) e separa uma\n",
        "# parte para treinamento.\n",
        "trnPaths = list(paths.list_images(conf[\"faces_dataset\"]))\n",
        "trnPaths = random.sample(trnPaths, int(len(trnPaths) * conf[\"percent_gt_images\"]))\n",
        "print(\"[INFO] describing training ROIs...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV7itGjPm4S4"
      },
      "source": [
        "# cria um elemento de progress bar só para acompanhar a evolução do processo.\n",
        "widgets = [\"Extraindo: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
        "pbar = progressbar.ProgressBar(maxval=len(trnPaths), widgets=widgets).start()\n",
        "# loop nos caminhos de arquivos de treinamento\n",
        "for (i, trnPath) in enumerate(trnPaths):\n",
        "  image = cv2.imread(trnPath)\n",
        "  # converte para escala de cinza\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  # define um ID para a imagem baseada no nome dela.\n",
        "  imageID = trnPath[trnPath.rfind(\"_\") + 1:].replace(\".jpg\", \"\")\n",
        "  # carrega a anotação (bounding box) da imagem\n",
        "  p = \"{}/annotation_{}.mat\".format(conf[\"image_data\"], imageID)\n",
        "  bb = io.loadmat(p)[\"box_coord\"][0]\n",
        "  # extrai o ROI da imagem (corta só o objeto de interesse) usando a função definida\n",
        "  # antes.\n",
        "  roi = roi_extract(image, bb, padding=conf[\"offset\"], dstSize=tuple(conf[\"window_dim\"]))\n",
        "\n",
        "  # realiza o flip na imagem para aumentar a quantidade de dados disponíveis.\n",
        "\n",
        "  rois = (roi, cv2.flip(roi, 1))\n",
        "\n",
        "  # loop em cada ROI\n",
        "  for roi in rois:\n",
        "    # extrai as características do ROI e atualiza a lista de features e rótulos\n",
        "    features = feature.hog(roi, conf['orientations'], pixels_per_cell=conf['pixels_per_cell'],\n",
        "        cells_per_block=conf['cells_per_block'], transform_sqrt=True)\n",
        "    data.append(features)\n",
        "    labels.append(1)\n",
        "  # atualiza a barra de progresso\n",
        "  pbar.update(i)\n",
        "pbar.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2pKUlSloST9"
      },
      "source": [
        "image = cv2.imread(list(paths.list_images(conf[\"faces_dataset\"]))[0])\n",
        "cv2_imshow(image)\n",
        "cv2_imshow(cv2.flip(image,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRpUMCZabSWn"
      },
      "source": [
        "Uma vez tratado todo o conjunto de exemplos positivos da nossa base de dados, precisamos extrair as características das imagens negativas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga4R3npqKh1g"
      },
      "source": [
        "from sklearn.feature_extraction.image import extract_patches_2d\n",
        "\n",
        "# define o número de imagens negativas a serem utilizadas\n",
        "conf['num_distraction_images'] = 500\n",
        "# define o número de pedaços da imagem negativa que deve ser extraído\n",
        "conf['num_distractions_per_image'] = 10\n",
        "\n",
        "# pega as imagens negativas\n",
        "dstPaths = list(paths.list_images(conf['image_distractions']))\n",
        "widgets = [\"Extraindo: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
        "pbar = progressbar.ProgressBar(maxval=conf[\"num_distraction_images\"], widgets=widgets).start()\n",
        "# loop em uma quantidade de imagens negativas\n",
        "for i in np.arange(0, conf['num_distraction_images']):\n",
        "  # aleatoriamente seleciona uma imagem do conjunto negativo\n",
        "  image = cv2.imread(random.choice(dstPaths))\n",
        "  # converte para escala de cinza\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  # extrai pedaços aleatórios da imagem\n",
        "  patches = extract_patches_2d(image, tuple(conf[\"window_dim\"]),\n",
        "    max_patches=conf[\"num_distractions_per_image\"])\n",
        "  # loop em cada pedaço\n",
        "  for patch in patches:\n",
        "    # extrai as características do pedaço e atualiza a lista de dados e rótulos.\n",
        "    features = feature.hog(patch, conf['orientations'], pixels_per_cell=conf['pixels_per_cell'],\n",
        "        cells_per_block=conf['cells_per_block'], transform_sqrt=True)\n",
        "    data.append(features)\n",
        "    labels.append(-1)\n",
        "  # atualiza a barra de progresso\n",
        "  pbar.update(i)\n",
        "pbar.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JIij1PudIN_"
      },
      "source": [
        "A última coisa a fazer é colocar todas as features extraídas no banco de dados construído."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJOg9ygPalwt"
      },
      "source": [
        "conf['features_path'] = '/gdrive/MyDrive/Mestrado/VisaoComputacional/outputs/faces_features.hdf5'\n",
        "print('[INFO] dumping features e rótulos para o arquivo...')\n",
        "dump_dataset(data, labels, conf['features_path'], 'features')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF3gDFledPH4"
      },
      "source": [
        "Depois de conseguir extrair as características do nosso conjunto de dados, incluindo exemplos positivos e negativos, vamos agora para o treinamento do classificador.\n",
        "\n",
        "Vamos primeiro carregar o nosso conjunto de dados a partir do banco criado anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAVjhryna-b7"
      },
      "source": [
        "print(\"[INFO] carregando dataset...\")\n",
        "(data, labels) = load_dataset(conf['features_path'], 'features')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hprZPT9dGhuy"
      },
      "source": [
        "Aqui vamos executar o processo de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jUUW9n7bK0v"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "import pickle\n",
        "\n",
        "# local para armazenar o classificador após treinado\n",
        "conf['classifier_path'] = '/gdrive/MyDrive/Mestrado/VisaoComputacional/outputs/classificador.cpickle'\n",
        "\n",
        "print(\"[INFO] treinando o classificador...\")\n",
        "\n",
        "model = SVC(kernel=\"linear\", C=0.8, probability=True, random_state=42)\n",
        "model.fit(data, labels)\n",
        "# armazenando o classificador no arquivo\n",
        "print(\"[INFO] dumping classificador...\")\n",
        "f = open(conf[\"classifier_path\"], \"wb\")\n",
        "f.write(pickle.dumps(model))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsfU0fW8GniU"
      },
      "source": [
        "A função abaixo é utilizada para fazer a detecção de um objeto na imagem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDnfg4FaCSGO"
      },
      "source": [
        "def detect(image, model, winDim, winStep=4, pyramidScale=2.5, minProb=0.7):\n",
        "# inicializa a lista de bounding boxes e probabilidades associadas\n",
        "  boxes = []\n",
        "  probs = []\n",
        "\n",
        "  # loop na pirâmide de imagens    \n",
        "  for layer in pyramid(image, scale=pyramidScale, minSize=winDim):\n",
        "    # determina a escala atual da pirâmide\n",
        "    scale = image.shape[0] / float(layer.shape[0])\n",
        "    # loop da janela deslizante em todas as imagens da pirâmide\n",
        "    for (x, y, window) in sliding_window(layer, winStep, winDim):\n",
        "      # pega as dimensões da janela\n",
        "      (winH, winW) = window.shape[:2]\n",
        "      # garante que as dimensões da janela se enquadram com os valores fornecidas\n",
        "      # para a janela deslizante      \n",
        "      if winH == winDim[1] and winW == winDim[0]:\n",
        "        # extrai as características HOG da janela atual e classifca para determinar\n",
        "        # se o objeto de interesse está presente ou não.        \n",
        "        \n",
        "        features = feature.hog(window, conf['orientations'], pixels_per_cell=conf['pixels_per_cell'],\n",
        "        cells_per_block=conf['cells_per_block'], transform_sqrt=True).reshape(1, -1)\n",
        "\n",
        "        #features = desc.describe(window).reshape(1, -1)\n",
        "        \n",
        "        prob = model.predict_proba(features)[0][1]\n",
        "        # verifica se o classificador encontrou um objeto com probabilidade suficiente        \n",
        "        if prob > minProb:\n",
        "          # calcula a coordenada (x,y) do bounding box usando a escala atual da imagem          \n",
        "          (startX, startY) = (int(scale * x), int(scale * y))\n",
        "          endX = int(startX + (scale * winW))\n",
        "          endY = int(startY + (scale * winH))\n",
        "          # atualiza a lista de bounding boxes e probabilidades          \n",
        "          boxes.append((startX, startY, endX, endY))\n",
        "          probs.append(prob)\n",
        "  # retorna uma tupla de bounding boxes e probabilidades\n",
        "  return (boxes, probs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmSvVYZ8G6Wy"
      },
      "source": [
        "Um problema de fazer a detecção dessa maneira é que múltiplos *bounding box* aparecem no resultado do teste, para evitar isso, vamos utilizar a técnica *non-maxima supression*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMx5pjkmXVS2"
      },
      "source": [
        "def non_max_suppression(boxes, probs, overlapThresh):\n",
        "  # se não existem bbs, retornar uma lista vazia\n",
        "  if len(boxes) == 0:\n",
        "    return []\n",
        "  # converte os valores dos bbs para float já que haverá operações de divisão.\n",
        "  if boxes.dtype.kind == \"i\":\n",
        "    boxes = boxes.astype(\"float\")\n",
        "  \n",
        "  # inicializa uma lista para guardar os bbs selecionados\n",
        "  pick = []\n",
        "  # pega as coordenadas dos bbs\n",
        "  x1 = boxes[:, 0]\n",
        "  y1 = boxes[:, 1]\n",
        "  x2 = boxes[:, 2]\n",
        "  y2 = boxes[:, 3]\n",
        "  # calcula a área dos bbs e os ordena pela probabilidade associada a cada um.\n",
        "  area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "  idxs = np.argsort(probs)\n",
        "\n",
        "  # loop pela lista de indices ordenadas\n",
        "  while len(idxs) > 0:\n",
        "    # pega o último indice da lista (maior probabilidade) e adiciona a lista de valores selecionados.\n",
        "    last = len(idxs) - 1\n",
        "    i = idxs[last]\n",
        "    pick.append(i)\n",
        "\n",
        "    # encontra os maiores valores de (x,y) para ser o início do bounding box e os\n",
        "    # menores valores (x,y) para ser o fim do bounding box.\n",
        "    xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
        "    yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
        "    xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
        "    yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
        "    # calcula a largura e altura do bounding box\n",
        "    w = np.maximum(0, xx2 - xx1 + 1)\n",
        "    h = np.maximum(0, yy2 - yy1 + 1)\n",
        "    # calcula a taxa de sobreposição\n",
        "    overlap = (w * h) / area[idxs[:last]]\n",
        "    # deleta todos os índices que possuem sobreposição maior que limiar\n",
        "    idxs = np.delete(idxs, np.concatenate(([last],\n",
        "      np.where(overlap > overlapThresh)[0])))\n",
        "  # retorna apenas os bounding boxes selecionados\n",
        "  return boxes[pick].astype(\"int\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K1HBOihHfU2"
      },
      "source": [
        "Finalmente, vamos fazer os testes para a deteção de faces nas imagens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFnNggs0XbxX"
      },
      "source": [
        "import imutils\n",
        "\n",
        "conf['overlap_thresh'] = 0.3\n",
        "\n",
        "# definindo mais um parâmetro para as janelas deslizantes\n",
        "conf['window_step'] = 4\n",
        "\n",
        "# definindo a probabilidade mínima para o classificador considerar que existe um\n",
        "# objeto na imagem\n",
        "conf['min_probability'] = 0.7\n",
        "\n",
        "# define a escala da pirâmide de imagens\n",
        "conf['pyramid_scale'] = 2.5\n",
        "\n",
        "model = pickle.loads(open(conf['classifier_path'], 'rb').read())\n",
        "\n",
        "image = cv2.imread('/gdrive/MyDrive/Mestrado/VisaoComputacional/Faces/image_0141.jpg')\n",
        "image = imutils.resize(image, width=min(260, image.shape[1]))\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# detect objects in the image and apply non-maxima suppression to the bounding boxes\n",
        "(boxes, probs) = detect(gray, model, conf[\"window_dim\"], winStep=conf[\"window_step\"],\n",
        "\tpyramidScale=conf[\"pyramid_scale\"], minProb=conf[\"min_probability\"])\n",
        "\n",
        "pick = non_max_suppression(np.array(boxes), probs, conf[\"overlap_thresh\"])\n",
        "orig = image.copy()\n",
        "\n",
        "# loop over the original bounding boxes and draw them\n",
        "for (startX, startY, endX, endY) in boxes:\n",
        "\tcv2.rectangle(orig, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
        "\n",
        "# loop over the allowed bounding boxes and draw them\n",
        "for (startX, startY, endX, endY) in pick:\n",
        "\tcv2.rectangle(image, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
        "# show the output images\n",
        "cv2_imshow(orig)\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}